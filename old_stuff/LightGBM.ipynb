{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc #garbage collector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import random \n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from helper import utility\n",
    "import importlib\n",
    "importlib.reload(utility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Zillow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data set provided\n",
    "prop_2016 = utility.load_data('data/properties_2016.csv')\n",
    "prop_2017 = utility.load_data('data/properties_2017.csv')\n",
    "train_2016 = utility.load_data('data/train_2016_v2.csv' , ['transactiondate'])\n",
    "train_2017 = utility.load_data('data/train_2017.csv', ['transactiondate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the prop dataset with its corresponding train datasets on their parcelid\n",
    "# left join is used so that all properties without logerror will be ignored\n",
    "training_2016 = utility.merge_data(train_2016, prop_2016, 'parcelid')\n",
    "training_2017 = utility.merge_data(train_2017, prop_2017, 'parcelid')\n",
    "\n",
    "\n",
    "\n",
    "# Data across the 2 years are combined into one data frame for processing at later stages\n",
    "training_all = pd.concat([training_2016, training_2017] , ignore_index=True)\n",
    "properties_all = pd.concat([prop_2016, prop_2017], ignore_index=True) \n",
    "\n",
    "\n",
    "training_all\n",
    "#properties_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and Drop any duplicates in the training dataset\n",
    "# Duplicates are those which have the same parcelid and transactiondate\n",
    "\n",
    "training_all.shape\n",
    "utility.check_duplicates(training_all)\n",
    "training_all = utility.drop_dups(training_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are no duplicates in the dataset so far\n",
    "training_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Target Variable - logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = training_all['logerror']\n",
    "\n",
    "\n",
    "target_y.hist(bins=50, figsize=(8,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers that are more than 2.5 std away from mean\n",
    "upper_threshold = target_y.mean() + (2.5*target_y.std())\n",
    "lower_threshold = target_y.mean() - (2.5*target_y.std())\n",
    "\n",
    "\n",
    "# Remove data that have their target y value as outliers\n",
    "training_all = training_all[training_all['logerror'] < upper_threshold]\n",
    "training_all = training_all[training_all['logerror'] > lower_threshold]\n",
    "training_all.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Feature to the dataset\n",
    "# Add Day, Month, Year and which quarter the transaction was done\n",
    "training_all = utility.add_dmy_feature(training_all)\n",
    "training_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility.print_percent_missing(training_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns that have missing threashold greater than 95%\n",
    "MISSING_THRESHOLD = 0.97\n",
    "col_to_drop = utility.get_col_to_drop_missing(training_all, 0.95)\n",
    "col_to_drop += utility.get_col_to_drop_non_unique(training_all)\n",
    "\n",
    "# Other columns to exlude to prepare for training dataset\n",
    "exclude_list = [\"parcelid\" , \"logerror\" , 'propertyzoningdesc']\n",
    "\n",
    "remaining_col = []\n",
    "for col in training_all.columns:\n",
    "    if col not in col_to_drop and col not in exclude_list:\n",
    "        remaining_col.append(col)\n",
    "        print(col)\n",
    "\n",
    "print('Number of columns left:', len(remaining_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with Categorical Values\n",
    "# Convert categorical values to 'category' type for some columns\n",
    "\n",
    "category_list= ['heatingorsystemtypeid','propertylandusetypeid', 'storytypeid', \n",
    "               'airconditioningtypeid', 'architecturalstyletypeid','typeconstructiontypeid'\n",
    "                'buildingclasstypeid', 'quarter', 'day', 'transaction_year', 'transaction_day']\n",
    "for col in training_all.columns:\n",
    "    if col in category_list: \n",
    "        utility.float_to_categorical(training_all, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert float64 values to float32 values \n",
    "# for col in training_all.columns: \n",
    "#     if training_all[col].dtype.name == 'float64': \n",
    "#         training_all[col] = training_all[col].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns, only keep the remaining columns \n",
    "training_cleaned=training_all\n",
    "\n",
    "for col in training_all: \n",
    "    if col not in remaining_col: \n",
    "        training_cleaned=training_cleaned.drop([col], axis=1)\n",
    "\n",
    "training_cleaned.head()\n",
    "# for col in training_cleaned: \n",
    "#     training_cleaned[col] = training_cleaned[col].astype('category')\n",
    "# training_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = []\n",
    "for i,col in enumerate(training_cleaned.columns): \n",
    "    if col in category_list: \n",
    "        category_index.append(i)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving LightGBM models\n",
    "def save_models(models):\n",
    "    for i, model in enumerate(models):\n",
    "        model.save_model('checkpoints/lgb_' + str(i))\n",
    "    print(\"Saved {} LightGBM models to files.\".format(len(models)))\n",
    "\n",
    "# Load LightGBM models from files\n",
    "def load_models(paths):\n",
    "    models = []\n",
    "    for path in paths:\n",
    "        model = lgb.Booster(model_file=path)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_label = training_all.logerror\n",
    "lgb_label.head()\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ord_enc = OrdinalEncoder()\n",
    "lgb_X=ord_enc.fit_transform(training_cleaned)\n",
    "#Transform to Numpy matrices \n",
    "#lgb_X = training_cleaned.values\n",
    "lgb_y = lgb_label.values\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(lgb_X, lgb_y, test_size=0.2)\n",
    "\n",
    "# Remove outlier examples from X_train and y_train; Keep them in X_val and y_val for proper cross-validation\n",
    "outlier_threshold = 0.4\n",
    "mask = (abs(y_train) <= outlier_threshold)\n",
    "X_train = X_train[mask, :]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'\n",
    "params['num_threads'] = 4  # set to number of real CPU cores for best performance\n",
    "\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['num_boost_round'] = 500\n",
    "params['learning_rate'] = 0.003  # shrinkage_rate\n",
    "#params['early_stopping_rounds'] = 30  # Early stopping based on validation set performance\n",
    "\n",
    "# Control tree growing\n",
    "params['num_leaves'] = 127  # max number of leaves in one tree (default 31)\n",
    "params['min_data'] = 150  # min_data_in_leaf\n",
    "params['min_hessian'] = 0.001  # min_sum_hessian_in_leaf (default 1e-3)\n",
    "params['max_depth'] = -1  # limit the max depth of tree model, defult -1 (no limit)\n",
    "params['max_bin'] = 255  # max number of bins that feature values are bucketed in (small -> less overfitting, default 255)\n",
    "params['sub_feature'] = 0.5    # feature_fraction (small values => use very different submodels)\n",
    "\n",
    "# Row subsampling (speed up training and alleviate overfitting)\n",
    "params['bagging_fraction'] = 0.7\n",
    "params['bagging_freq'] = 50  # perform bagging at every k iteration\n",
    "\n",
    "# Constraints on categorical features\n",
    "params['min_data_per_group'] = 100  # minimal number of data per categorical group (default 100)\n",
    "params['cat_smooth'] = 15.0  # reduce effect of noises in categorical features, especially for those with few data (default 10.0)\n",
    "\n",
    "# Regularization (default 0.0)\n",
    "params['lambda_l1'] = 0.0\n",
    "params['lambda_l2'] = 0.0\n",
    "\n",
    "# Random seeds (keep default values)\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_set = lgb.Dataset(X_train, label=y_train, feature_name=remaining_col)\n",
    "lgb_valid_set = lgb.Dataset(X_val, label=y_val, feature_name=remaining_col)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set,\n",
    "                valid_sets=[lgb_train_set, lgb_valid_set], valid_names=['train', 'val'],\n",
    "                categorical_feature=category_index)\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(\"Train score: {}\".format(abs(model.predict(X_train) - y_train).mean() * 100))\n",
    "print(\"Val score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, height=0.8, figsize=(12.5, 12.5), ignore_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: make sure the model score is reasonable on a small portion of the data\n",
    "print(\"score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2016=utility.load_data('data/properties_2016.csv')\n",
    "features_2017=utility.load_data('data/properties_2017.csv')\n",
    "\n",
    "# Other columns to exlude to prepare for training dataset\n",
    "exclude_list = [\"logerror\" , 'propertyzoningdesc']\n",
    "\n",
    "remaining_col = []\n",
    "for col in training_all.columns:\n",
    "    if col not in col_to_drop and col not in exclude_list:\n",
    "        remaining_col.append(col)\n",
    "\n",
    "for col in features_2016.columns:\n",
    "    if col not in remaining_col:\n",
    "        features_2016 = features_2016.drop([col], axis=1)\n",
    "\n",
    "for col in features_2017.columns:\n",
    "    if col not in remaining_col:\n",
    "        features_2017 = features_2017.drop([col], axis=1)\n",
    "\n",
    "\n",
    "badfeatures = ['hashottuborspa', 'propertycountylandusecode', 'propertyzoningdesc', 'fireplaceflag', 'taxdelinquencyflag']\n",
    "\n",
    "for col in features_2016.columns:\n",
    "    if col in badfeatures:\n",
    "        features_2016 = features_2016.drop([col], axis=1)\n",
    "\n",
    "for col in features_2017.columns:\n",
    "    if col in badfeatures:\n",
    "        features_2017 = features_2017.drop([col], axis=1)\n",
    "\n",
    "for col in features_2016.columns:\n",
    "    if col in category_list:\n",
    "        utility.float_to_categorical(features_2016, col)\n",
    "\n",
    "for col in features_2017.columns:\n",
    "    if col in category_list:\n",
    "        utility.float_to_categorical(features_2017, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Helper method that prepares 2016 and 2017 properties features for inference\n",
    "\"\"\"\n",
    "def transform_test_features(features_2016, features_2017):  \n",
    "    test_features_2016=features_2016\n",
    "    test_features_2017=features_2017\n",
    "      \n",
    "    test_features_2016['year'] = 0\n",
    "    test_features_2017['year'] = 1\n",
    "    \n",
    "    # 11 & 12 lead to unstable results, probably due to the fact that there are few training examples for them\n",
    "    test_features_2016['month'] = 10\n",
    "    test_features_2017['month'] = 10\n",
    "    \n",
    "    test_features_2016['quarter'] = 4\n",
    "    test_features_2017['quarter'] = 4\n",
    "    \n",
    "    return test_features_2016, test_features_2017\n",
    "\n",
    "\"\"\"\n",
    "    Helper method that makes predictions on the test set and exports results to csv file\n",
    "    'models' is a list of models for ensemble prediction (len=1 means using just a single model)\n",
    "\"\"\"\n",
    "def predict_and_export(models, features_2016, features_2017, file_name):\n",
    "    # Construct DataFrame for prediction results\n",
    "    submission_2016 = pd.DataFrame()\n",
    "    submission_2017 = pd.DataFrame()\n",
    "    submission_2016['ParcelId'] = features_2016.parcelid\n",
    "    submission_2017['ParcelId'] = features_2017.parcelid\n",
    "    \n",
    "    test_features_2016, test_features_2017 = transform_test_features(features_2016, features_2017)\n",
    "    \n",
    "    pred_2016, pred_2017 = [], []\n",
    "    for i, model in enumerate(models):\n",
    "        print(\"Start model {} (2016)\".format(i))\n",
    "        pred_2016.append(model.predict(test_features_2016, predict_disable_shape_check=True))\n",
    "        print(\"Start model {} (2017)\".format(i))\n",
    "        pred_2017.append(model.predict(test_features_2017, predict_disable_shape_check=True))\n",
    "    \n",
    "    # Take average across all models\n",
    "    mean_pred_2016 = np.mean(pred_2016, axis=0)\n",
    "    mean_pred_2017 = np.mean(pred_2017, axis=0)\n",
    "    \n",
    "    submission_2016['201610'] = [float(format(x, '.4f')) for x in mean_pred_2016]\n",
    "    submission_2016['201611'] = submission_2016['201610']\n",
    "    submission_2016['201612'] = submission_2016['201610']\n",
    "\n",
    "    submission_2017['201710'] = [float(format(x, '.4f')) for x in mean_pred_2017]\n",
    "    submission_2017['201711'] = submission_2017['201710']\n",
    "    submission_2017['201712'] = submission_2017['201710']\n",
    "    \n",
    "    submission = submission_2016.merge(how='inner', right=submission_2017, on='ParcelId')\n",
    "    \n",
    "    print(\"Length of submission DataFrame: {}\".format(len(submission)))\n",
    "    print(\"Submission header:\")\n",
    "    print(submission.head())\n",
    "    submission.to_csv(file_name, index=False)\n",
    "    return submission, pred_2016, pred_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'lgb.csv'\n",
    "submission, pred_2016, pred_2017 = predict_and_export([model], features_2016, features_2017, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
