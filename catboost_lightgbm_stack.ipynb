{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc #garbage collector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from helper import utility\n",
    "import importlib\n",
    "importlib.reload(utility)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data set provided\n",
    "prop_2016 = utility.load_data('data/properties_2016.csv')\n",
    "prop_2017 = utility.load_data('data/properties_2017.csv')\n",
    "train_2016 = utility.load_data('data/train_2016_v2.csv' , ['transactiondate'])\n",
    "train_2017 = utility.load_data('data/train_2017.csv', ['transactiondate'])\n",
    "\n",
    "# Combining the prop dataset with its corresponding train datasets on their parcelid\n",
    "# left join is used so that all properties without logerror will be ignored\n",
    "training_2016 = utility.merge_data(train_2016, prop_2016, 'parcelid')\n",
    "training_2017 = utility.merge_data(train_2017, prop_2017, 'parcelid')\n",
    "\n",
    "\n",
    "\n",
    "# Data across the 2 years are combined into one data frame for processing at later stages\n",
    "training_all = pd.concat([training_2016, training_2017] , ignore_index=True)\n",
    "properties_all = pd.concat([prop_2016, prop_2017], ignore_index=True) \n",
    "\n",
    "\n",
    "training_all\n",
    "#properties_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cleaned=drop_features(training_all)\n",
    "training_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_indexes = get_categorical_indices(training_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_cleaned.values\n",
    "y = (training_all.logerror.astype(np.float32)).values\n",
    "X_train, X_val, y_train, y_val = prepare_training(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lightgbm parameters\n",
    "params = {}\n",
    "\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'\n",
    "params['num_threads'] = 4  # set to number of real CPU cores for best performance\n",
    "\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['num_boost_round'] = 1250\n",
    "params['learning_rate'] = 0.003  # shrinkage_rate\n",
    "params['early_stopping_rounds'] = 30  # Early stopping based on validation set performance\n",
    "\n",
    "# Control tree growing\n",
    "params['num_leaves'] = 127  # max number of leaves in one tree (default 31)\n",
    "params['min_data'] = 150  # min_data_in_leaf\n",
    "params['min_hessian'] = 0.001  # min_sum_hessian_in_leaf (default 1e-3)\n",
    "params['max_depth'] = -1  # limit the max depth of tree model, defult -1 (no limit)\n",
    "params['max_bin'] = 255  # max number of bins that feature values are bucketed in (small -> less overfitting, default 255)\n",
    "params['sub_feature'] = 0.5    # feature_fraction (small values => use very different submodels)\n",
    "\n",
    "# Row subsampling (speed up training and alleviate overfitting)\n",
    "params['bagging_fraction'] = 0.7\n",
    "params['bagging_freq'] = 50  # perform bagging at every k iteration\n",
    "\n",
    "# Constraints on categorical features\n",
    "params['min_data_per_group'] = 100  # minimal number of data per categorical group (default 100)\n",
    "params['cat_smooth'] = 15.0  # reduce effect of noises in categorical features, especially for those with few data (default 10.0)\n",
    "\n",
    "# Regularization (default 0.0)\n",
    "params['lambda_l1'] = 0.0\n",
    "params['lambda_l2'] = 0.0\n",
    "\n",
    "# Random seeds (keep default values)\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Lightgbm\n",
    "lgb_train_set = lgb.Dataset(X_train, label=y_train, feature_name=training_cleaned)\n",
    "lgb_valid_set = lgb.Dataset(X_val, label=y_val, feature_name=training_cleaned)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set,\n",
    "                valid_sets=[lgb_train_set, lgb_valid_set], valid_names=['train', 'val'],\n",
    "                categorical_feature=categorical_indexes)\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(\"Train score: {}\".format(abs(model.predict(X_train) - y_train).mean() * 100))\n",
    "print(\"Val score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LightGBM feature importance\n",
    "lgb.plot_importance(model, height=0.8, figsize=(12.5, 12.5), ignore_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM on all given training data (preparing for submission)\n",
    "del params['early_stopping_rounds']\n",
    "\n",
    "lgb_X,lgb_y=remove_outliers(X,y,training_cleaned)\n",
    "\n",
    "lgb_train_set = lgb.Dataset(lgb_X, label=lgb_y, feature_name=training_cleaned)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set, categorical_feature=categorical_indexes)\n",
    "\n",
    "# Sanity check: make sure the model score is reasonable on a small portion of the data\n",
    "print(\"score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'submission/final_lgb_single.csv'\n",
    "submission, pred_2016, pred_2017 = predict_and_export([model], prop_2016, prop_2017, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ensemble model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (if any) from training data\n",
    "lgb_X,lgb_y=remove_outliers(X,y,training_cleaned)\n",
    "\n",
    "lgb_train_set = lgb.Dataset(lgb_X, label=lgb_y, feature_name=training_cleaned)\n",
    "\n",
    "# Train multiple models\n",
    "bags = 5\n",
    "models = []\n",
    "for i in range(bags):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    params['seed'] = i\n",
    "    np.random.seed(42)\n",
    "    random.seed(36)\n",
    "    model = lgb.train(params, lgb_train_set, categorical_feature=categorical_indexes)\n",
    "    models.append(model)\n",
    "    \n",
    "# Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_val) - y_val).mean() * 100))\n",
    "\n",
    "# Save the trained models to disk\n",
    "save_models(models)\n",
    "\n",
    "models = load_lightgbm_models(['checkpoints/lgb_' + str(i) for i in range(5)])  # load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and export results\n",
    "file_name = 'submission/final_lgb_ensemble_x5.csv'\n",
    "submission, pred_2016, pred_2017 = predict_and_export(models, prop_2016, prop_2017, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_single = pd.read_csv('submission/final_lgb_single.csv')\n",
    "catboost_x8 = pd.read_csv('submission/final_catboost_ensemble_x8.csv')\n",
    "print(\"Finished Loading the prediction results.\")\n",
    "\n",
    "weight = 0.7\n",
    "stack = pd.DataFrame()\n",
    "stack['ParcelId'] = lgb_single['ParcelId']\n",
    "for col in ['201610', '201611', '201612', '201710', '201711', '201712']:\n",
    "    stack[col] = weight * catboost_x8[col] + (1 - weight) * lgb_single[col]\n",
    "\n",
    "print(stack.head())\n",
    "stack.to_csv('submission/final_stack.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
