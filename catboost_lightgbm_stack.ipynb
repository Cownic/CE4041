{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # garbage collector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from helper import utility as ut\n",
    "import importlib\n",
    "\n",
    "importlib.reload(ut)\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset - Common for both Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_2016 = ut.load_properties_data(\"clean_data/prop_2016_clean.csv\")\n",
    "prop_2017 = ut.load_properties_data(\"clean_data/prop_2017_clean.csv\")\n",
    "train = ut.load_properties_data(\"clean_data/train_combined.csv\")\n",
    "\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns which do not perform well when we input to the catboost model\n",
    "catboost_features = ut.drop_features(train)\n",
    "print(\"Number of features for CatBoost: {}\".format(len(catboost_features.columns)))\n",
    "catboost_features.head(5)\n",
    "\n",
    "# Prepare feature list for catboost model\n",
    "categorical_features = [\n",
    "    \"airconditioningtypeid\",\n",
    "    \"heatingorsystemtypeid\",\n",
    "    \"propertylandusetypeid\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"quarter\",\n",
    "    \"buildingclasstypeid\",\n",
    "]\n",
    "for col in catboost_features.columns:\n",
    "    if col in categorical_features:\n",
    "        catboost_features[col] = catboost_features[col].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and cross-validation data\n",
    "catboost_label = train.logerror.astype(np.float32)\n",
    "print(catboost_label.head())\n",
    "\n",
    "# Transform to Numpy matrices\n",
    "catboost_X = catboost_features.values\n",
    "catboost_y = catboost_label.values\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    catboost_X, catboost_y, test_size=0.2, random_state=99\n",
    ")\n",
    "ut.remove_outliers(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify feature names and categorical features for CatBoost\n",
    "categorical_indices = ut.get_categorical_indices(\n",
    "    catboost_features, categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.remove_outliers(catboost_X, catboost_y)\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    loss_function=\"MAE\",\n",
    "    eval_metric=\"MAE\",\n",
    "    nan_mode=\"Min\",\n",
    "    random_seed=99,\n",
    "    iterations=1000,\n",
    "    learning_rate=0.015,\n",
    "    border_count=254,\n",
    "    max_depth=6,\n",
    "    random_strength=1,\n",
    "    l2_leaf_reg=5,\n",
    "    bagging_temperature=1,\n",
    "    verbose=True,\n",
    ")\n",
    "model.fit(catboost_X, catboost_y, cat_features=categorical_indices, verbose=False)\n",
    "\n",
    "# Sanity check: score on a small portion of the dataset\n",
    "print(\"sanity check score: {}\".format(abs(model.predict(X_val) - y_val).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"submission/final_catboost_single.csv\"\n",
    "submission, pred_2016, pred_2017 = ut.predict_and_generate_csv(\n",
    "    [model], prop_2016, prop_2017, file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost with 4x ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_features = ut.drop_features(train)\n",
    "print(\"Number of features for Lightgbm: {}\".format(len(lightgbm_features.columns)))\n",
    "lightgbm_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and cross-validation data\n",
    "lightgbm_label = train.logerror.astype(np.float32)\n",
    "print(lightgbm_label.head())\n",
    "\n",
    "# Transform to Numpy matrices\n",
    "lightgbm_X = lightgbm_features.values\n",
    "lightgbm_y = lightgbm_label.values\n",
    "\n",
    "# Perform shuffled train/test split\n",
    "np.random.seed(42)\n",
    "random.seed(10)\n",
    "X_train, X_val, y_train, y_val = train_test_split(lightgbm_X, lightgbm_y, test_size=0.2 , random_state=99)\n",
    "\n",
    "a,b=ut.remove_outliers(X_train, y_train)\n",
    "X_train=a\n",
    "y_train=b\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_val shape: {}\".format(X_val.shape))\n",
    "print(\"y_val shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify feature names and categorical features for Lightgbm\n",
    "categorical_features = ['airconditioningtypeid', 'heatingorsystemtypeid', 'propertylandusetypeid', 'year', 'month', 'quarter','buildingclasstypeid']\n",
    "categorical_indices = ut.get_categorical_indices(lightgbm_features, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "rounds = 8\n",
    "models = []\n",
    "for i in range(rounds):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"MAE\",\n",
    "        eval_metric=\"MAE\",\n",
    "        nan_mode=\"Min\",\n",
    "        random_seed=99,\n",
    "        iterations=1000,\n",
    "        learning_rate=0.015,\n",
    "        border_count=254,\n",
    "        max_depth=6,\n",
    "        random_strength=1,\n",
    "        l2_leaf_reg=5,\n",
    "        bagging_temperature=1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    model.fit(catboost_X, catboost_y, cat_features=categorical_indices, verbose=True)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions and export results\n",
    "file_name = 'submission/final_catboost_ensemble_x4.csv'\n",
    "submission, pred_2016, pred_2017 = ut.predict_and_generate_csv(models, prop_2016, prop_2017, file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightgbm parameters\n",
    "params = {}\n",
    "\n",
    "params[\"objective\"] = \"regression\"\n",
    "params[\"metric\"] = \"mae\"\n",
    "params[\"num_threads\"] = 4  # set to number of real CPU cores for best performance\n",
    "\n",
    "params[\"boosting_type\"] = \"gbdt\"\n",
    "params[\"num_boost_round\"] = 1250\n",
    "params[\"learning_rate\"] = 0.003  # shrinkage_rate\n",
    "params[\n",
    "    \"early_stopping_rounds\"\n",
    "] = 30  # Early stopping based on validation set performance\n",
    "\n",
    "# Control tree growing\n",
    "params[\"num_leaves\"] = 127  # max number of leaves in one tree (default 31)\n",
    "params[\"min_data\"] = 150  # min_data_in_leaf\n",
    "params[\"min_hessian\"] = 0.001  # min_sum_hessian_in_leaf (default 1e-3)\n",
    "params[\"max_depth\"] = -1  # limit the max depth of tree model, defult -1 (no limit)\n",
    "params[\n",
    "    \"max_bin\"\n",
    "] = 255  # max number of bins that feature values are bucketed in (small -> less overfitting, default 255)\n",
    "params[\n",
    "    \"sub_feature\"\n",
    "] = 0.5  # feature_fraction (small values => use very different submodels)\n",
    "\n",
    "# Row subsampling (speed up training and alleviate overfitting)\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 50  # perform bagging at every k iteration\n",
    "\n",
    "# Constraints on categorical features\n",
    "params[\n",
    "    \"min_data_per_group\"\n",
    "] = 100  # minimal number of data per categorical group (default 100)\n",
    "params[\n",
    "    \"cat_smooth\"\n",
    "] = 15.0  # reduce effect of noises in categorical features, especially for those with few data (default 10.0)\n",
    "\n",
    "# Regularization (default 0.0)\n",
    "params[\"lambda_l1\"] = 0.0\n",
    "params[\"lambda_l2\"] = 0.0\n",
    "\n",
    "# Random seeds (keep default values)\n",
    "params[\"feature_fraction_seed\"] = 2\n",
    "params[\"bagging_seed\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Single Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Lightgbm\n",
    "feature_names = [s for s in lightgbm_features.columns]\n",
    "lgb_train_set = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)\n",
    "lgb_valid_set = lgb.Dataset(X_val, label=y_val, feature_name=feature_names)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set,\n",
    "                valid_sets=[lgb_train_set, lgb_valid_set], valid_names=['train', 'val'],\n",
    "                categorical_feature=categorical_indices)\n",
    "\n",
    "# Evaluate on train and validation sets\n",
    "print(\"Train score: {}\".format(abs(model.predict(X_train) - y_train).mean() * 100))\n",
    "print(\"Val score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LightGBM feature importance\n",
    "lgb.plot_importance(model, height=0.8, figsize=(12.5, 12.5), ignore_zero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM on all given training data (preparing for submission)\n",
    "#del params['early_stopping_rounds']\n",
    "\n",
    "a,b=ut.remove_outliers(lightgbm_X,lightgbm_y)\n",
    "lightgbm_X=a\n",
    "lightgbm_y=b\n",
    "\n",
    "lgb_train_set = lgb.Dataset(lightgbm_X, label=lightgbm_y, feature_name=feature_names)\n",
    "print(\"lightgbm_X: {}\".format(lightgbm_X.shape))\n",
    "print(\"lightgbm_y: {}\".format(lightgbm_y.shape))\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(36)\n",
    "model = lgb.train(params, lgb_train_set, categorical_feature=categorical_indices)\n",
    "\n",
    "# Sanity check: make sure the model score is reasonable on a small portion of the data\n",
    "print(\"score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'submission/final_lgb_single.csv'\n",
    "submission, pred_2016, pred_2017 = ut.predict_and_export([model], prop_2016, prop_2017, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ensemble model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers (if any) from training data\n",
    "a,b=ut.remove_outliers(lightgbm_X,lightgbm_y)\n",
    "lightgbm_X=a\n",
    "lightgbm_y=b\n",
    "\n",
    "lgb_train_set = lgb.Dataset(lightgbm_X, label=lightgbm_y, feature_name=feature_names)\n",
    "\n",
    "# Train multiple models\n",
    "bags = 5\n",
    "models = []\n",
    "for i in range(bags):\n",
    "    print(\"Start training model {}\".format(i))\n",
    "    params[\"seed\"] = i\n",
    "    np.random.seed(42)\n",
    "    random.seed(36)\n",
    "    model = lgb.train(params, lgb_train_set, categorical_feature=categorical_indices)\n",
    "    models.append(model)\n",
    "\n",
    "# Sanity check (make sure scores on a small portion of the dataset are reasonable)\n",
    "for i, model in enumerate(models):\n",
    "    print(\"model {}: {}\".format(i, abs(model.predict(X_val) - y_val).mean() * 100))\n",
    "\n",
    "# Save the trained models to disk\n",
    "ut.save_models(models,'lightgbm')\n",
    "\n",
    "models = ut.load_lightgbm_models(['checkpoints/lightgbm_' + str(i) for i in range(5)])  # load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and export results\n",
    "file_name = 'submission/final_lgb_ensemble_x5.csv'\n",
    "submission, pred_2016, pred_2017 = ut.predict_and_export(models, prop_2016, prop_2017, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_single = pd.read_csv('submission/final_lgb_single.csv')\n",
    "catboost_x8 = pd.read_csv('submission/final_catboost_single.csv')\n",
    "print(\"Finished Loading the prediction results.\")\n",
    "\n",
    "weight = 0.7\n",
    "stack = pd.DataFrame()\n",
    "stack[\"ParcelId\"] = lgb_single[\"ParcelId\"]\n",
    "for col in [\"201610\", \"201611\", \"201612\", \"201710\", \"201711\", \"201712\"]:\n",
    "    stack[col] = weight * catboost_x8[col] + (1 - weight) * lgb_single[col]\n",
    "\n",
    "print(stack.head())\n",
    "stack.to_csv(\"submission/final_stack.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
